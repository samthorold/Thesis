{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>PERMNO</th><th>date</th><th>SHRCD</th><th>EXCHCD</th><th>RET</th></tr></thead><tbody><tr><th>1</th><td>10006</td><td>19630603</td><td>10</td><td>1</td><td>0.015464</td></tr><tr><th>2</th><td>10006</td><td>19630604</td><td>10</td><td>1</td><td>0.005076</td></tr><tr><th>3</th><td>10006</td><td>19630605</td><td>10</td><td>1</td><td>0.0</td></tr><tr><th>4</th><td>10006</td><td>19630606</td><td>10</td><td>1</td><td>0.002525</td></tr></tbody></table>"
      ],
      "text/plain": [
       "4×5 DataFrames.DataFrame\n",
       "│ Row │ PERMNO │ date     │ SHRCD │ EXCHCD │ RET      │\n",
       "├─────┼────────┼──────────┼───────┼────────┼──────────┤\n",
       "│ 1   │ 10006  │ 19630603 │ 10    │ 1      │ 0.015464 │\n",
       "│ 2   │ 10006  │ 19630604 │ 10    │ 1      │ 0.005076 │\n",
       "│ 3   │ 10006  │ 19630605 │ 10    │ 1      │ 0.0      │\n",
       "│ 4   │ 10006  │ 19630606 │ 10    │ 1      │ 0.002525 │"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = CSV.read(\"C:/Data/CRSP/20180325_CRSP_daily_19630601_20171231.csv\"; rows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"C:/Data/CRSP/20180325_CRSP_daily_19630601_20171231.csv\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 71.608935 seconds (194.13 M allocations: 5.064 GiB, 60.85% gc time)\n"
     ]
    }
   ],
   "source": [
    "# `nullable` true by default, but there missing values further down than\n",
    "# CSV.read checks and a MissingException is raised without nullable=true\n",
    "@time df = CSV.read(fname; nullable=true, types=[String, String, String, String, String]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrames.DataFrame, (19406009, 5))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof(df), size(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>PERMNO</th><th>date</th><th>SHRCD</th><th>EXCHCD</th><th>RET</th></tr></thead><tbody><tr><th>1</th><td>10006</td><td>19630603</td><td>10</td><td>1</td><td>0.015464</td></tr><tr><th>2</th><td>10006</td><td>19630604</td><td>10</td><td>1</td><td>0.005076</td></tr><tr><th>3</th><td>10006</td><td>19630605</td><td>10</td><td>1</td><td>0.000000</td></tr><tr><th>4</th><td>10006</td><td>19630606</td><td>10</td><td>1</td><td>0.002525</td></tr><tr><th>5</th><td>10006</td><td>19630607</td><td>10</td><td>1</td><td>-0.012594</td></tr><tr><th>6</th><td>10006</td><td>19630610</td><td>10</td><td>1</td><td>0.000000</td></tr></tbody></table>"
      ],
      "text/plain": [
       "6×5 DataFrames.DataFrame\n",
       "│ Row │ PERMNO │ date     │ SHRCD │ EXCHCD │ RET       │\n",
       "├─────┼────────┼──────────┼───────┼────────┼───────────┤\n",
       "│ 1   │ 10006  │ 19630603 │ 10    │ 1      │ 0.015464  │\n",
       "│ 2   │ 10006  │ 19630604 │ 10    │ 1      │ 0.005076  │\n",
       "│ 3   │ 10006  │ 19630605 │ 10    │ 1      │ 0.000000  │\n",
       "│ 4   │ 10006  │ 19630606 │ 10    │ 1      │ 0.002525  │\n",
       "│ 5   │ 10006  │ 19630607 │ 10    │ 1      │ -0.012594 │\n",
       "│ 6   │ 10006  │ 19630610 │ 10    │ 1      │ 0.000000  │"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERMNO\n",
      "Summary Stats:\n",
      "Length:         19406009\n",
      "Type:           Union{Missings.Missing, String}\n",
      "Number Unique:  5363\n",
      "Number Missing: 0\n",
      "% Missing:      0.000000\n",
      "\n",
      "date\n",
      "Summary Stats:\n",
      "Length:         19406009\n",
      "Type:           Union{Missings.Missing, String}\n",
      "Number Unique:  13744\n",
      "Number Missing: 0\n",
      "% Missing:      0.000000\n",
      "\n",
      "SHRCD\n",
      "Summary Stats:\n",
      "Length:         19406009\n",
      "Type:           Union{Missings.Missing, String}\n",
      "Number Unique:  2\n",
      "Number Missing: 0\n",
      "% Missing:      0.000000\n",
      "\n",
      "EXCHCD\n",
      "Summary Stats:\n",
      "Length:         19406009\n",
      "Type:           Union{Missings.Missing, String}\n",
      "Number Unique:  1\n",
      "Number Missing: 0\n",
      "% Missing:      0.000000\n",
      "\n",
      "RET\n",
      "Summary Stats:\n",
      "Length:         19406009\n",
      "Type:           Union{Missings.Missing, String}\n",
      "Number Unique:  218005\n",
      "Number Missing: 8730\n",
      "% Missing:      0.044986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365 days"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Date(\"20140101\", \"yyyymmdd\") - Date(\"20130101\", \"yyyymmdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2013, 1, 1 month)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upper case returns a period\n",
    "Dates.year(Date(\"20130101\", \"yyyymmdd\")), Dates.month(Date(\"20130101\", \"yyyymmdd\")),  Dates.Month(Date(\"20130101\", \"yyyymmdd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013-01-31"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dates.lastdayofmonth(Date(\"20130116\", \"yyyymmdd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dates.lastdayofmonth(Date(\"20130116\", \"yyyymmdd\")) == Date(\"20130116\", \"yyyymmdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2013-01-01, 2013-02-01)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floor(Date(\"20130116\", \"yyyymmdd\"), Dates.Month), ceil(Date(\"20130116\", \"yyyymmdd\"), Dates.Month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-11-22"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dates.tonext(Date(2018,3,30)) do x\n",
    "    # Return true on the 4th Thursday of November (Thanksgiving)\n",
    "    Dates.dayofweek(x) == Dates.Thursday &&\n",
    "    Dates.dayofweekofmonth(x) == 4 &&\n",
    "    Dates.month(x) == Dates.November\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Date,1}:\n",
       " 2014-04-08\n",
       " 2014-05-13\n",
       " 2014-06-10\n",
       " 2014-07-08\n",
       " 2014-08-12\n",
       " 2014-09-09\n",
       " 2014-10-14\n",
       " 2014-11-11"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pittsburgh street cleaning; Every 2nd Tuesday from April to November\n",
    "# Date range from January 1st, 2014 to January 1st, 2015\n",
    "dr = Dates.Date(2014):Dates.Date(2015);\n",
    "\n",
    "filter(dr) do x\n",
    "   Dates.dayofweek(x) == Dates.Tue &&\n",
    "   Dates.April <= Dates.month(x) <= Dates.Nov &&\n",
    "   Dates.dayofweekofmonth(x) == 2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15-element Array{Date,1}:\n",
       " 2013-01-01\n",
       " 2013-01-02\n",
       " 2013-01-03\n",
       " 2013-01-04\n",
       " 2013-01-05\n",
       " 2013-01-06\n",
       " 2013-01-07\n",
       " 2013-01-08\n",
       " 2013-01-09\n",
       " 2013-01-10\n",
       " 2013-01-11\n",
       " 2013-01-12\n",
       " 2013-01-13\n",
       " 2013-01-14\n",
       " 2013-01-15"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(Date(\"20130101\", \"yyyymmdd\"):Date(\"20130115\", \"yyyymmdd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13-element Array{Date,1}:\n",
       " 2013-01-01\n",
       " 2013-02-01\n",
       " 2013-03-01\n",
       " 2013-04-01\n",
       " 2013-05-01\n",
       " 2013-06-01\n",
       " 2013-07-01\n",
       " 2013-08-01\n",
       " 2013-09-01\n",
       " 2013-10-01\n",
       " 2013-11-01\n",
       " 2013-12-01\n",
       " 2014-01-01"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(Date(\"20130101\", \"yyyymmdd\"):Dates.Month(1):Date(\"20140101\", \"yyyymmdd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19397279,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(collect(skipmissing(df[:RET])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2320"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(collect(skipmissing(df[:RET])).==\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(collect(skipmissing(df[:RET])).==\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(collect(skipmissing(df[:RET])).==\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Missings.EachSkipMissing{Array{Union{Missings.Missing, String},1}}(Union{Missings.Missing, String}[\"0.015464\", \"0.005076\", \"0.000000\", \"0.002525\", \"-0.012594\"])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipmissing(df[:RET][1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{String,1}:\n",
       " \"0.015464\" \n",
       " \"0.005076\" \n",
       " \"0.000000\" \n",
       " \"0.002525\" \n",
       " \"-0.012594\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(skipmissing(df[:RET][1:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       "  0.015464\n",
       "  0.005076\n",
       "  0.0     \n",
       "  0.002525\n",
       " -0.012594"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse.(Float64, collect(skipmissing(df[:RET][1:5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`CSV.read(fullpath::Union{AbstractString,IO}, sink::Type{T}=DataFrame, args...; kwargs...)` => `typeof(sink)`\n",
       "\n",
       "`CSV.read(fullpath::Union{AbstractString,IO}, sink::Data.Sink; kwargs...)` => `Data.Sink`\n",
       "\n",
       "parses a delimited file into a Julia structure (a DataFrame by default, but any valid `Data.Sink` may be requested).\n",
       "\n",
       "Minimal error-reporting happens w/ `CSV.read` for performance reasons; for problematic csv files, try [`CSV.validate`](@ref) which takes exact same arguments as `CSV.read` and provides much more information for why reading the file failed.\n",
       "\n",
       "Positional arguments:\n",
       "\n",
       "  * `fullpath`; can be a file name (string) or other `IO` instance\n",
       "  * `sink::Type{T}`; `DataFrame` by default, but may also be other `Data.Sink` types that support streaming via `Data.Field` interface; note that the method argument can be the *type* of `Data.Sink`, plus any required arguments the sink may need (`args...`).                   or an already constructed `sink` may be passed (2nd method above)\n",
       "\n",
       "Keyword Arguments:\n",
       "\n",
       "  * `delim::Union{Char,UInt8}`: how fields in the file are delimited; default `','`\n",
       "  * `quotechar::Union{Char,UInt8}`: the character that indicates a quoted field that may contain the `delim` or newlines; default `'\"'`\n",
       "  * `escapechar::Union{Char,UInt8}`: the character that escapes a `quotechar` in a quoted field; default `'\\'`\n",
       "  * `null::String`: indicates how NULL values are represented in the dataset; default `\"\"`\n",
       "  * `dateformat::Union{AbstractString,Dates.DateFormat}`: how dates/datetimes are represented in the dataset; default `Base.Dates.ISODateTimeFormat`\n",
       "  * `decimal::Union{Char,UInt8}`: character to recognize as the decimal point in a float number, e.g. `3.14` or `3,14`; default `'.'`\n",
       "  * `truestring`: string to represent `true::Bool` values in a csv file; default `\"true\"`. Note that `truestring` and `falsestring` cannot start with the same character.\n",
       "  * `falsestring`: string to represent `false::Bool` values in a csv file; default `\"false\"`\n",
       "  * `header`: column names can be provided manually as a complete Vector{String}, or as an Int/AbstractRange which indicates the row/rows that contain the column names\n",
       "  * `datarow::Int`: specifies the row on which the actual data starts in the file; by default, the data is expected on the next row after the header row(s); for a file without column names (header), specify `datarow=1`\n",
       "  * `types`: column types can be provided manually as a complete Vector{Type}, or in a Dict to reference individual columns by name or number\n",
       "  * `nullable::Bool`: indicates whether values can be nullable or not; `true` by default. If set to `false` and missing values are encountered, a `Data.NullException` will be thrown\n",
       "  * `footerskip::Int`: indicates the number of rows to skip at the end of the file\n",
       "  * `rows_for_type_detect::Int=100`: indicates how many rows should be read to infer the types of columns\n",
       "  * `rows::Int`: indicates the total number of rows to read from the file; by default the file is pre-parsed to count the # of rows; `-1` can be passed to skip a full-file scan, but the `Data.Sink` must be set up to account for a potentially unknown # of rows\n",
       "  * `use_mmap::Bool=true`: whether the underlying file will be mmapped or not while parsing; note that on Windows machines, the underlying file will not be \"deletable\" until Julia GC has run (can be run manually via `gc()`) due to the use of a finalizer when reading the file.\n",
       "  * `append::Bool=false`: if the `sink` argument provided is an existing table, `append=true` will append the source's data to the existing data instead of doing a full replace\n",
       "  * `transforms::Dict{Union{String,Int},Function}`: a Dict of transforms to apply to values as they are parsed. Note that a column can be specified by either number or column name.\n",
       "  * `transpose::Bool=false`: when reading the underlying csv data, rows should be treated as columns and columns as rows, thus the resulting dataset will be the \"transpose\" of the actual csv data.\n",
       "  * `categorical::Bool=true`: read string column as a `CategoricalArray` ([ref](https://github.com/JuliaData/CategoricalArrays.jl)), as long as the % of unique values seen during type detection is less than 67%. This will dramatically reduce memory use in cases where the number of unique values is small.\n",
       "  * `weakrefstrings::Bool=true`: whether to use [`WeakRefStrings`](https://github.com/quinnj/WeakRefStrings.jl) package to speed up file parsing; can only be `=true` for the `Sink` objects that support `WeakRefStringArray` columns. Note that `WeakRefStringArray` still returns regular `String` elements.\n",
       "\n",
       "Example usage:\n",
       "\n",
       "```\n",
       "julia> dt = CSV.read(\"bids.csv\")\n",
       "7656334×9 DataFrames.DataFrame\n",
       "│ Row     │ bid_id  │ bidder_id                               │ auction │ merchandise      │ device      │\n",
       "├─────────┼─────────┼─────────────────────────────────────────┼─────────┼──────────────────┼─────────────┤\n",
       "│ 1       │ 0       │ \"8dac2b259fd1c6d1120e519fb1ac14fbqvax8\" │ \"ewmzr\" │ \"jewelry\"        │ \"phone0\"    │\n",
       "│ 2       │ 1       │ \"668d393e858e8126275433046bbd35c6tywop\" │ \"aeqok\" │ \"furniture\"      │ \"phone1\"    │\n",
       "│ 3       │ 2       │ \"aa5f360084278b35d746fa6af3a7a1a5ra3xe\" │ \"wa00e\" │ \"home goods\"     │ \"phone2\"    │\n",
       "...\n",
       "```\n",
       "\n",
       "Other example invocations may include:\n",
       "\n",
       "```julia\n",
       "# read in a tab-delimited file\n",
       "CSV.read(file; delim='\t')\n",
       "\n",
       "# read in a comma-delimited file with null values represented as '\\N', such as a MySQL export\n",
       "CSV.read(file; null=\"\\N\")\n",
       "\n",
       "# read a csv file that happens to have column names in the first column, and grouped data in rows instead of columns\n",
       "CSV.read(file; transpose=true)\n",
       "\n",
       "# manually provided column names; must match # of columns of data in file\n",
       "# this assumes there is no header row in the file itself, so data parsing will start at the very beginning of the file\n",
       "CSV.read(file; header=[\"col1\", \"col2\", \"col3\"])\n",
       "\n",
       "# manually provided column names, even though the file itself has column names on the first row\n",
       "# `datarow` is specified to ensure data parsing occurs at correct location\n",
       "CSV.read(file; header=[\"col1\", \"col2\", \"col3\"], datarow=2)\n",
       "\n",
       "# types provided manually; as a vector, must match length of columns in actual data\n",
       "CSV.read(file; types=[Int, Int, Float64])\n",
       "\n",
       "# types provided manually; as a Dict, can specify columns by # or column name\n",
       "CSV.read(file; types=Dict(3=>Float64, 6=>String))\n",
       "CSV.read(file; types=Dict(\"col3\"=>Float64, \"col6\"=>String))\n",
       "\n",
       "# manually provided # of rows; if known beforehand, this will improve parsing speed\n",
       "# this is also a way to limit the # of rows to be read in a file if only a sample is needed\n",
       "CSV.read(file; rows=10000)\n",
       "\n",
       "# for data files, `file` and `file2`, with the same structure, read both into a single DataFrame\n",
       "# note that `df` is used as a 2nd argument in the 2nd call to `CSV.read` and the keyword argument\n",
       "# `append=true` is passed\n",
       "df = CSV.read(file)\n",
       "df = CSV.read(file2, df; append=true)\n",
       "\n",
       "# manually construct a `CSV.Source` once, then stream its data to both a DataFrame\n",
       "# and SQLite table `sqlite_table` in the SQLite database `db`\n",
       "# note the use of `CSV.reset!` to ensure the `source` can be streamed from again\n",
       "source = CSV.Source(file)\n",
       "df1 = CSV.read(source, DataFrame)\n",
       "CSV.reset!(source)\n",
       "db = SQLite.DB()\n",
       "sq1 = CSV.read(source, SQLite.Sink, db, \"sqlite_table\")\n",
       "```\n"
      ],
      "text/plain": [
       "`CSV.read(fullpath::Union{AbstractString,IO}, sink::Type{T}=DataFrame, args...; kwargs...)` => `typeof(sink)`\n",
       "\n",
       "`CSV.read(fullpath::Union{AbstractString,IO}, sink::Data.Sink; kwargs...)` => `Data.Sink`\n",
       "\n",
       "parses a delimited file into a Julia structure (a DataFrame by default, but any valid `Data.Sink` may be requested).\n",
       "\n",
       "Minimal error-reporting happens w/ `CSV.read` for performance reasons; for problematic csv files, try [`CSV.validate`](@ref) which takes exact same arguments as `CSV.read` and provides much more information for why reading the file failed.\n",
       "\n",
       "Positional arguments:\n",
       "\n",
       "  * `fullpath`; can be a file name (string) or other `IO` instance\n",
       "  * `sink::Type{T}`; `DataFrame` by default, but may also be other `Data.Sink` types that support streaming via `Data.Field` interface; note that the method argument can be the *type* of `Data.Sink`, plus any required arguments the sink may need (`args...`).                   or an already constructed `sink` may be passed (2nd method above)\n",
       "\n",
       "Keyword Arguments:\n",
       "\n",
       "  * `delim::Union{Char,UInt8}`: how fields in the file are delimited; default `','`\n",
       "  * `quotechar::Union{Char,UInt8}`: the character that indicates a quoted field that may contain the `delim` or newlines; default `'\"'`\n",
       "  * `escapechar::Union{Char,UInt8}`: the character that escapes a `quotechar` in a quoted field; default `'\\'`\n",
       "  * `null::String`: indicates how NULL values are represented in the dataset; default `\"\"`\n",
       "  * `dateformat::Union{AbstractString,Dates.DateFormat}`: how dates/datetimes are represented in the dataset; default `Base.Dates.ISODateTimeFormat`\n",
       "  * `decimal::Union{Char,UInt8}`: character to recognize as the decimal point in a float number, e.g. `3.14` or `3,14`; default `'.'`\n",
       "  * `truestring`: string to represent `true::Bool` values in a csv file; default `\"true\"`. Note that `truestring` and `falsestring` cannot start with the same character.\n",
       "  * `falsestring`: string to represent `false::Bool` values in a csv file; default `\"false\"`\n",
       "  * `header`: column names can be provided manually as a complete Vector{String}, or as an Int/AbstractRange which indicates the row/rows that contain the column names\n",
       "  * `datarow::Int`: specifies the row on which the actual data starts in the file; by default, the data is expected on the next row after the header row(s); for a file without column names (header), specify `datarow=1`\n",
       "  * `types`: column types can be provided manually as a complete Vector{Type}, or in a Dict to reference individual columns by name or number\n",
       "  * `nullable::Bool`: indicates whether values can be nullable or not; `true` by default. If set to `false` and missing values are encountered, a `Data.NullException` will be thrown\n",
       "  * `footerskip::Int`: indicates the number of rows to skip at the end of the file\n",
       "  * `rows_for_type_detect::Int=100`: indicates how many rows should be read to infer the types of columns\n",
       "  * `rows::Int`: indicates the total number of rows to read from the file; by default the file is pre-parsed to count the # of rows; `-1` can be passed to skip a full-file scan, but the `Data.Sink` must be set up to account for a potentially unknown # of rows\n",
       "  * `use_mmap::Bool=true`: whether the underlying file will be mmapped or not while parsing; note that on Windows machines, the underlying file will not be \"deletable\" until Julia GC has run (can be run manually via `gc()`) due to the use of a finalizer when reading the file.\n",
       "  * `append::Bool=false`: if the `sink` argument provided is an existing table, `append=true` will append the source's data to the existing data instead of doing a full replace\n",
       "  * `transforms::Dict{Union{String,Int},Function}`: a Dict of transforms to apply to values as they are parsed. Note that a column can be specified by either number or column name.\n",
       "  * `transpose::Bool=false`: when reading the underlying csv data, rows should be treated as columns and columns as rows, thus the resulting dataset will be the \"transpose\" of the actual csv data.\n",
       "  * `categorical::Bool=true`: read string column as a `CategoricalArray` ([ref](https://github.com/JuliaData/CategoricalArrays.jl)), as long as the % of unique values seen during type detection is less than 67%. This will dramatically reduce memory use in cases where the number of unique values is small.\n",
       "  * `weakrefstrings::Bool=true`: whether to use [`WeakRefStrings`](https://github.com/quinnj/WeakRefStrings.jl) package to speed up file parsing; can only be `=true` for the `Sink` objects that support `WeakRefStringArray` columns. Note that `WeakRefStringArray` still returns regular `String` elements.\n",
       "\n",
       "Example usage:\n",
       "\n",
       "```\n",
       "julia> dt = CSV.read(\"bids.csv\")\n",
       "7656334×9 DataFrames.DataFrame\n",
       "│ Row     │ bid_id  │ bidder_id                               │ auction │ merchandise      │ device      │\n",
       "├─────────┼─────────┼─────────────────────────────────────────┼─────────┼──────────────────┼─────────────┤\n",
       "│ 1       │ 0       │ \"8dac2b259fd1c6d1120e519fb1ac14fbqvax8\" │ \"ewmzr\" │ \"jewelry\"        │ \"phone0\"    │\n",
       "│ 2       │ 1       │ \"668d393e858e8126275433046bbd35c6tywop\" │ \"aeqok\" │ \"furniture\"      │ \"phone1\"    │\n",
       "│ 3       │ 2       │ \"aa5f360084278b35d746fa6af3a7a1a5ra3xe\" │ \"wa00e\" │ \"home goods\"     │ \"phone2\"    │\n",
       "...\n",
       "```\n",
       "\n",
       "Other example invocations may include:\n",
       "\n",
       "```julia\n",
       "# read in a tab-delimited file\n",
       "CSV.read(file; delim='\t')\n",
       "\n",
       "# read in a comma-delimited file with null values represented as '\\N', such as a MySQL export\n",
       "CSV.read(file; null=\"\\N\")\n",
       "\n",
       "# read a csv file that happens to have column names in the first column, and grouped data in rows instead of columns\n",
       "CSV.read(file; transpose=true)\n",
       "\n",
       "# manually provided column names; must match # of columns of data in file\n",
       "# this assumes there is no header row in the file itself, so data parsing will start at the very beginning of the file\n",
       "CSV.read(file; header=[\"col1\", \"col2\", \"col3\"])\n",
       "\n",
       "# manually provided column names, even though the file itself has column names on the first row\n",
       "# `datarow` is specified to ensure data parsing occurs at correct location\n",
       "CSV.read(file; header=[\"col1\", \"col2\", \"col3\"], datarow=2)\n",
       "\n",
       "# types provided manually; as a vector, must match length of columns in actual data\n",
       "CSV.read(file; types=[Int, Int, Float64])\n",
       "\n",
       "# types provided manually; as a Dict, can specify columns by # or column name\n",
       "CSV.read(file; types=Dict(3=>Float64, 6=>String))\n",
       "CSV.read(file; types=Dict(\"col3\"=>Float64, \"col6\"=>String))\n",
       "\n",
       "# manually provided # of rows; if known beforehand, this will improve parsing speed\n",
       "# this is also a way to limit the # of rows to be read in a file if only a sample is needed\n",
       "CSV.read(file; rows=10000)\n",
       "\n",
       "# for data files, `file` and `file2`, with the same structure, read both into a single DataFrame\n",
       "# note that `df` is used as a 2nd argument in the 2nd call to `CSV.read` and the keyword argument\n",
       "# `append=true` is passed\n",
       "df = CSV.read(file)\n",
       "df = CSV.read(file2, df; append=true)\n",
       "\n",
       "# manually construct a `CSV.Source` once, then stream its data to both a DataFrame\n",
       "# and SQLite table `sqlite_table` in the SQLite database `db`\n",
       "# note the use of `CSV.reset!` to ensure the `source` can be streamed from again\n",
       "source = CSV.Source(file)\n",
       "df1 = CSV.read(source, DataFrame)\n",
       "CSV.reset!(source)\n",
       "db = SQLite.DB()\n",
       "sq1 = CSV.read(source, SQLite.Sink, db, \"sqlite_table\")\n",
       "```\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?CSV.read"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
